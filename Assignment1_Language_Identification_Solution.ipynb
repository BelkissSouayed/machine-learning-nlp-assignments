{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BelkissSouayed/machine-learning-nlp-assignments/blob/main/Assignment1_Language_Identification_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBqq-G2daMW7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "import string\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "lcNG_T-pPJzL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEqfrri6aMW8"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "!gdown 1QP6YuwdKFNUPpvhOaAcvv2Pcp4JMbIRs # x_train\n",
        "!gdown 1QVo7PZAdiZKzifK8kwhEr_umosiDCUx6 # x_test\n",
        "!gdown 1QbBeKcmG2ZyAEFB3AKGTgSWQ1YEMn2jl # y_train\n",
        "!gdown 1QaZj6bI7_78ymnN8IpSk4gVvg-C9fA6X # y_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'x_train.txt') as f:\n",
        "    x_train = f.read().splitlines()\n",
        "with open(f'y_train.txt') as f:\n",
        "    y_train = f.read().splitlines()\n",
        "with open(f'x_test.txt') as f:\n",
        "    x_test = f.read().splitlines()\n",
        "with open(f'y_test.txt') as f:\n",
        "    y_test = f.read().splitlines()"
      ],
      "metadata": {
        "id": "oU9VZeEEabMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4PijB0TaMW8"
      },
      "outputs": [],
      "source": [
        "# combine x_train and y_train into one dataframe\n",
        "train_df = pd.DataFrame({'text': x_train, 'label': y_train})\n",
        "# write train_df to csv with tab as separator\n",
        "train_df.to_csv('train_df.csv', index=False, sep='\\t')\n",
        "# comibne x_test and y_test into one dataframe\n",
        "test_df = pd.DataFrame({'text': x_test, 'label': y_test})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "-TjW9Z2OytMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_96MFk6daMW9"
      },
      "outputs": [],
      "source": [
        "# get list of all labels\n",
        "labels = train_df['label'].unique().tolist()\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Peek at some examples from the training data\n",
        "print(train_df.sample(10))\n"
      ],
      "metadata": {
        "id": "ixeEbNJ3-kfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Challenges Noticed:\n",
        "Non-Latin Scripts: Texts in scripts like Arabic, Amharic, etc.\n",
        "Similar Scripts: Languages like Swedish and Danish using similar scripts.\n",
        "Special Characters: Presence of special characters and diacritics.\n",
        "Short Text Lengths: Some texts may be too short to provide enough context.\n",
        "Code-Switching: Potential mixing of multiple languages in a single text."
      ],
      "metadata": {
        "id": "-Y2ybPrC_CvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count instances per label in the training set\n",
        "train_label_counts = train_df['label'].value_counts()\n",
        "\n",
        "# Count instances per label in the test set\n",
        "test_label_counts = test_df['label'].value_counts()\n",
        "\n",
        "print(\"Training label counts:\\n\", train_label_counts)\n",
        "print(\"Test label counts:\\n\", test_label_counts)\n"
      ],
      "metadata": {
        "id": "z2PZGBOt_JIU",
        "outputId": "21845b79-e884-453b-ee86-c2cbf3a5b26e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training label counts:\n",
            " label\n",
            "est    500\n",
            "eng    500\n",
            "vep    500\n",
            "sgs    500\n",
            "uig    500\n",
            "      ... \n",
            "lmo    500\n",
            "mya    500\n",
            "ilo    500\n",
            "csb    500\n",
            "ltz    500\n",
            "Name: count, Length: 235, dtype: int64\n",
            "Test label counts:\n",
            " label\n",
            "mwl    500\n",
            "uig    500\n",
            "tat    500\n",
            "nno    500\n",
            "new    500\n",
            "      ... \n",
            "frp    500\n",
            "krc    500\n",
            "mlg    500\n",
            "msa    500\n",
            "ckb    500\n",
            "Name: count, Length: 235, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 500 instances per label for both the training and test sets, indicating that each label is equally represented in both sets. This is evident from the following output:\n",
        "\n",
        "- Training Label Counts:\n",
        "Each label has 500 instances.\n",
        "Total number of unique labels: 235\n",
        "\n",
        "- Test Label Counts:\n",
        "Each label has 500 instances.\n",
        "Total number of unique labels: 235\n",
        "\n",
        "- =>The dataset is balanced since each label has an equal number of instances in both the training and test sets.\n",
        "- => Appropriate Train/Test Split: The train/test split appears appropriate because each label is equally represented in both sets."
      ],
      "metadata": {
        "id": "Hgey2q48_S2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Predefined set of mandatory language labels\n",
        "given_labels = set(['eng', 'deu', 'nld', 'dan', 'swe', 'nno', 'jpn'])\n",
        "\n",
        "# Get all unique labels present in the training dataset\n",
        "all_labels = set(train_df['label'].unique())\n",
        "\n",
        "# Randomly select 20 additional labels from the set of all labels, excluding the given mandatory labels\n",
        "target_labels = list(np.random.choice(list(all_labels - given_labels), 20, replace=False))\n",
        "\n",
        "# Combine the mandatory labels with the randomly selected additional labels\n",
        "target_labels += given_labels\n",
        "\n",
        "# Filter the training dataframe to include only the rows with labels in the target_labels list\n",
        "train_df = train_df[train_df['label'].isin(target_labels)]\n",
        "\n",
        "# Filter the test dataframe to include only the rows with labels in the target_labels list\n",
        "test_df = test_df[test_df['label'].isin(target_labels)]\n",
        "\n",
        "# Print the size of the filtered training and test datasets\n",
        "print(\"Filtered training data size:\", train_df.shape)\n",
        "print(\"Filtered test data size:\", test_df.shape)\n"
      ],
      "metadata": {
        "id": "gUz86fiFAM1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(20)"
      ],
      "metadata": {
        "id": "diR9sh7SASvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_fitted = LabelEncoder().fit(train_df['label'])"
      ],
      "metadata": {
        "id": "Ii1rGGIACuww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le_fitted.classes_"
      ],
      "metadata": {
        "id": "GMFBxB-MC1Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_dev, y_test = le_fitted.transform(train_df['label']), le_fitted.transform(test_df['label'])"
      ],
      "metadata": {
        "id": "W37cE1KbDDLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a suitable pipeline in sklearn to preprocess the data. Think about extending the feature space./ Train the following classifier: LogisticRegression/ find the optimal hyperparameter settings for the classifier, use sklearn’s GridSearchCV"
      ],
      "metadata": {
        "id": "RUYKbw1jDPxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Custom transformer for normalizing text\n",
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def _normalize_text(self, text):\n",
        "        \"\"\"Remove punctuation, replace newlines, and lowercase the text.\n",
        "\n",
        "        :param text: string\n",
        "        :return: normalized string\n",
        "        \"\"\"\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "        text = re.sub(r'\\n', r'', text)  # Remove newlines\n",
        "        # text = text.lower()  # Optionally lowercase the text\n",
        "        return text\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Apply normalization to each text in the input array.\n",
        "\n",
        "        :param X: array-like of strings\n",
        "        :return: numpy array of normalized strings\n",
        "        \"\"\"\n",
        "        texts = [self._normalize_text(text) for text in X]\n",
        "        return np.array(texts)\n",
        "\n",
        "# Custom transformer for extracting additional linguistic features\n",
        "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    vowels = set('aeiouäöüàéèëï')\n",
        "    consonants = set('bcdfghklmnlpqrstvwxyz')\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def _to_bigrams(self, text):\n",
        "        \"\"\"Generate bigrams from the input text.\n",
        "\n",
        "        :param text: string\n",
        "        :return: list of bigrams\n",
        "        \"\"\"\n",
        "        return [bg[0] + bg[1] for bg in zip(text, text[1:])]\n",
        "\n",
        "    def _get_vowel_consonant_ratio(self, text):\n",
        "        \"\"\"Calculate the vowel to consonant ratio in the text.\n",
        "\n",
        "        :param text: string\n",
        "        :return: float ratio of vowels to consonants\n",
        "        \"\"\"\n",
        "        vf, cf = 0, 0\n",
        "        for c in text.lower():\n",
        "            if c in self.vowels:\n",
        "                vf += 1\n",
        "            elif c in self.consonants:\n",
        "                cf += 1\n",
        "        return vf / (cf + 1)\n",
        "\n",
        "    def _get_capitalization_ratio(self, text):\n",
        "        \"\"\"Calculate the ratio of uppercase to total characters in the text.\n",
        "\n",
        "        :param text: string\n",
        "        :return: float ratio of uppercase characters\n",
        "        \"\"\"\n",
        "        up_count = sum(1 for c in text if c.isupper())\n",
        "        return up_count / (len(text) + 1)\n",
        "\n",
        "    def _get_double_char_freq(self, text):\n",
        "        \"\"\"Calculate the frequency of double characters in the text.\n",
        "\n",
        "        :param text: string\n",
        "        :return: int frequency of double characters\n",
        "        \"\"\"\n",
        "        return sum(1 for bg in self._to_bigrams(text) if bg[0] == bg[1])\n",
        "\n",
        "    def _extract_num_features(self, texts):\n",
        "        \"\"\"Extract numerical features from a list of texts.\n",
        "\n",
        "        :param texts: list of strings\n",
        "        :return: numpy array of numerical features\n",
        "        \"\"\"\n",
        "        num_features = []\n",
        "        for text in texts:\n",
        "            features = [\n",
        "                self._get_vowel_consonant_ratio(text),\n",
        "                self._get_capitalization_ratio(text),\n",
        "                self._get_double_char_freq(text)\n",
        "            ]\n",
        "            num_features.append(features)\n",
        "        return np.array(num_features)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the scaler on the extracted numerical features.\n",
        "\n",
        "        :param X: array-like of strings\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        numerical_features = self._extract_num_features(X)\n",
        "        self.scaler.fit(numerical_features)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Transform the input texts into scaled numerical features.\n",
        "\n",
        "        :param X: array-like of strings\n",
        "        :return: tuple (original texts, scaled numerical features)\n",
        "        \"\"\"\n",
        "        numerical_features = self._extract_num_features(X)\n",
        "        return X, self.scaler.transform(numerical_features)\n",
        "\n",
        "# Wrapper to combine vectorizer and additional features\n",
        "class VectorizerWrapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.countvec = TfidfVectorizer(ngram_range=(1, 2), max_features=500)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the vectorizer on the text data.\n",
        "\n",
        "        :param X: tuple (texts, numerical features)\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        texts, _ = X\n",
        "        self.countvec.fit(texts)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Transform the texts into TF-IDF features and combine with numerical features.\n",
        "\n",
        "        :param X: tuple (texts, numerical features)\n",
        "        :return: tuple (TF-IDF features, numerical features)\n",
        "        \"\"\"\n",
        "        texts, numerical_features = X\n",
        "        return self.countvec.transform(texts), numerical_features\n",
        "\n",
        "# Transformer to convert sparse matrix to dense array\n",
        "class MatrixToArrayConverter(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Convert sparse matrices to dense arrays.\n",
        "\n",
        "        :param X: tuple (sparse TF-IDF matrix, numerical features)\n",
        "        :return: tuple (dense TF-IDF array, numerical features)\n",
        "        \"\"\"\n",
        "        return X[0].toarray(), X[1]\n",
        "\n",
        "# Transformer to unify text and numerical features into a single array\n",
        "class MatrixUnifier(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Concatenate text features and numerical features into a single array.\n",
        "\n",
        "        :param X: tuple (TF-IDF features, numerical features)\n",
        "        :return: numpy array of combined features\n",
        "        \"\"\"\n",
        "        return np.concatenate([X[0], X[1]], axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "a3rk6dF3DQKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the hyperparameter grid for GridSearchCV\n",
        "clf_param_grid = {\n",
        "    'CLF__penalty': ['l2', 'l1'],  # Regularization types\n",
        "    'CLF__solver': ['liblinear', 'saga'],  # Solvers for optimization\n",
        "    'CLF__max_iter': [50, 100]  # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Create the pipeline with custom transformers and logistic regression\n",
        "pipe = Pipeline(steps=[\n",
        "    ('TextNormalizer', TextNormalizer()),  # Normalize the text\n",
        "    ('FeatureExtractor', FeatureExtractor()),  # Extract additional linguistic features\n",
        "    ('Vectorizer', VectorizerWrapper()),  # Vectorize text and combine with additional features\n",
        "    ('MatrixToArrayConverter', MatrixToArrayConverter()),  # Convert sparse matrix to dense array\n",
        "    ('MatrixUnifier', MatrixUnifier()),  # Unify text and numerical features\n",
        "    ('CLF', LogisticRegression())  # Logistic Regression classifier\n",
        "], verbose=True)\n",
        "\n",
        "# Set up GridSearchCV for hyperparameter tuning\n",
        "grid = GridSearchCV(pipe, n_jobs=1, param_grid=clf_param_grid, scoring='f1_micro', cv=2)\n",
        "grid.fit(train_df['text'].to_numpy(), y_train_encoded)\n",
        "\n",
        "# Best model from GridSearchCV\n",
        "best_model = grid.best_estimator_\n",
        "print(\"Best hyperparameters:\", grid.best_params_)\n",
        "print(\"Best cross-validation score:\", grid.best_score_)\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "y_pred = best_model.predict(test_df['text'].to_numpy())\n",
        "\n"
      ],
      "metadata": {
        "id": "cJsRFOQAFLGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_results = pd.DataFrame.from_dict(grid.cv_results_)\n",
        "grid_results.sort_values(by=[\"rank_test_score\"])\n"
      ],
      "metadata": {
        "id": "ssvvptJ8H0Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "preds = grid.best_estimator_.predict(test_df['text'].to_numpy())\n",
        "\n",
        "# Calculate F1 scores\n",
        "f1_micro = f1_score(preds, y_test_encoded, average='micro')\n",
        "f1_macro = f1_score(preds, y_test_encoded, average='macro')\n",
        "print(f'F1-micro-score on the test set: {f1_micro}')\n",
        "print(f'F1-macro-score on the test set: {f1_macro}')"
      ],
      "metadata": {
        "id": "4TaGaxDgH7Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of classes\n",
        "num_classes = len(le_fitted.classes_)\n",
        "\n",
        "# Function to create a confusion matrix manually\n",
        "def create_confusion_matrix(num_classes, preds, y_test):\n",
        "    \"\"\"Create confusion matrix 'by hand' since test set does not contain all labels.\"\"\"\n",
        "    df = pd.DataFrame(np.zeros((num_classes, num_classes), dtype=int))\n",
        "    for i, j in zip(preds, y_test):\n",
        "        df.iloc[i, j] += 1\n",
        "    df.columns = le_fitted.classes_\n",
        "    df.index = le_fitted.classes_\n",
        "    return df\n",
        "\n",
        "# Create confusion matrix\n",
        "confusion_df = create_confusion_matrix(num_classes, preds, y_test_encoded)\n",
        "print(confusion_df)\n",
        "\n",
        "# Identify the best model for interpretation\n",
        "model_to_interpret = grid.best_estimator_"
      ],
      "metadata": {
        "id": "kHJPs1YKIDxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z6dzqmu1Io-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install eli5\n"
      ],
      "metadata": {
        "id": "u_4nwJLxGjP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "from eli5 import show_weights\n",
        "# Extract the logistic regression model from the pipeline\n",
        "lr_model = model_to_interpret.named_steps['CLF']\n",
        "\n",
        "# Get the vectorizer model from the pipeline\n",
        "vec_model = model_to_interpret.named_steps['Vectorizer'].countvec\n",
        "\n",
        "# Define the target indices for specific languages\n",
        "target_indices = [np.where(le_fitted.classes_ == 'eng')[0][0],\n",
        "                  np.where(le_fitted.classes_ == 'jpn')[0][0],\n",
        "                  np.where(le_fitted.classes_ == 'swe')[0][0]]\n",
        "\n",
        "# Create feature names including the additional features\n",
        "make_new_feature_names = np.concatenate([\n",
        "    vec_model.get_feature_names_out(),\n",
        "    np.array([\"extra_feature_\" + str(i) for i in range(3)])\n",
        "], axis=-1)\n",
        "\n",
        "# Display the model weights using ELI5 for specified target languages\n",
        "show_weights(lr_model, top=(10, 10), feature_names=make_new_feature_names,\n",
        "             target_names=le_fitted.classes_, targets=['eng', 'swe', 'nno'])\n"
      ],
      "metadata": {
        "id": "iQKisfn2IxrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ablation study"
      ],
      "metadata": {
        "id": "_wht-Nn1JXk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Custom transformer to reduce text length\n",
        "class TextReducer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, max_len):\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def _reduce_text(self, text):\n",
        "        \"\"\"Reduce text length to a maximum of max_len characters.\"\"\"\n",
        "        if len(text) > self.max_len:\n",
        "            text = text[:self.max_len]\n",
        "        return text\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Apply text reduction to each text in the input array.\"\"\"\n",
        "        return [self._reduce_text(text) for text in X]\n",
        "\n",
        "# Re-create the best model using the best parameters from the grid search\n",
        "best_model = LogisticRegression(\n",
        "    solver=grid.best_params_['CLF__solver'],\n",
        "    penalty=grid.best_params_['CLF__penalty'],\n",
        "    max_iter=grid.best_params_['CLF__max_iter']\n",
        ")\n",
        "\n",
        "# Pipeline for text reduced to 500 characters\n",
        "pipe500 = Pipeline(steps=[\n",
        "    ('TextReducer', TextReducer(max_len=500)),\n",
        "    ('TextNormalizer', TextNormalizer()),\n",
        "    ('FeatureExtractor', FeatureExtractor()),\n",
        "    ('Vectorizer', VectorizerWrapper()),\n",
        "    ('MatrixToArrayConverter', MatrixToArrayConverter()),\n",
        "    ('MatrixUnifier', MatrixUnifier()),\n",
        "    ('CLF', best_model)\n",
        "], verbose=True)\n",
        "\n",
        "# Pipeline for text reduced to 100 characters\n",
        "pipe100 = Pipeline(steps=[\n",
        "    ('TextReducer', TextReducer(max_len=100)),\n",
        "    ('TextNormalizer', TextNormalizer()),\n",
        "    ('FeatureExtractor', FeatureExtractor()),\n",
        "    ('Vectorizer', VectorizerWrapper()),\n",
        "    ('MatrixToArrayConverter', MatrixToArrayConverter()),\n",
        "    ('MatrixUnifier', MatrixUnifier()),\n",
        "    ('CLF', best_model)\n",
        "], verbose=True)\n",
        "\n",
        "# Function to fit model and evaluate F1 score\n",
        "def fit_and_evaluate(pipeline, X_train, y_train, X_test, y_test):\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    preds = pipeline.predict(X_test)\n",
        "    return f1_score(preds, y_test, average='micro')\n",
        "\n",
        "# Fit and evaluate the model with text reduced to 500 characters\n",
        "f1_micro_500 = fit_and_evaluate(pipe500, train_df['text'].to_numpy(), y_train_encoded, test_df['text'].to_numpy(), y_test_encoded)\n",
        "print(f'F1-micro-score with text reduced to 500 characters: {f1_micro_500}')\n",
        "\n",
        "# Fit and evaluate the model with text reduced to 100 characters\n",
        "f1_micro_100 = fit_and_evaluate(pipe100, train_df['text'].to_numpy(), y_train_encoded, test_df['text'].to_numpy(), y_test_encoded)\n",
        "print(f'F1-micro-score with text reduced to 100 characters: {f1_micro_100}')\n",
        "\n",
        "# Evaluate the model with all characters (no reduction)\n",
        "pipe_all = Pipeline(steps=[\n",
        "    ('TextNormalizer', TextNormalizer()),\n",
        "    ('FeatureExtractor', FeatureExtractor()),\n",
        "    ('Vectorizer', VectorizerWrapper()),\n",
        "    ('MatrixToArrayConverter', MatrixToArrayConverter()),\n",
        "    ('MatrixUnifier', MatrixUnifier()),\n",
        "    ('CLF', best_model)\n",
        "], verbose=True)\n",
        "\n",
        "f1_micro_all = fit_and_evaluate(pipe_all, train_df['text'].to_numpy(), y_train_encoded, test_df['text'].to_numpy(), y_test_encoded)\n",
        "print(f'F1-micro-score with all characters: {f1_micro_all}')\n"
      ],
      "metadata": {
        "id": "Is86OAFxIyCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XwxXbDvGKj4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network\n"
      ],
      "metadata": {
        "id": "xjaQ5BtwKqfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install skorch\n"
      ],
      "metadata": {
        "id": "ue11RME9K1Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from skorch import NeuralNetClassifier\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Ensure reproducibility\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "\n",
        "# Custom text normalizer to preprocess text\n",
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def _normalize_text(self, text):\n",
        "        \"\"\"Remove punctuation, lowercase, pad with spaces.\"\"\"\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        text = re.sub(r'\\n', r'', text)\n",
        "        return text\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return [self._normalize_text(text) for text in X]\n",
        "\n",
        "# Custom feature extractor to add linguistic features\n",
        "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    vowels = set('aeiouäöüàéèëï')\n",
        "    consonants = set('bcdfghklmnlpqrstvwxyz')\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def _to_bigrams(self, text):\n",
        "        return [bg[0] + bg[1] for bg in zip(text, text[1:])]\n",
        "\n",
        "    def _get_vowel_consonant_ratio(self, text):\n",
        "        vf, cf = 0, 0\n",
        "        for c in text.lower():\n",
        "            if c in self.vowels:\n",
        "                vf += 1\n",
        "            elif c in self.consonants:\n",
        "                cf += 1\n",
        "        return vf / (cf + 1)\n",
        "\n",
        "    def _get_capitalization_ratio(self, text):\n",
        "        up_count = sum(1 for c in text if c.isupper())\n",
        "        return up_count / (len(text) + 1)\n",
        "\n",
        "    def _get_double_char_freq(self, text):\n",
        "        return sum(1 for bg in self._to_bigrams(text) if bg[0] == bg[1])\n",
        "\n",
        "    def _extract_num_features(self, texts):\n",
        "        num_features = []\n",
        "        for text in texts:\n",
        "            features = [\n",
        "                self._get_vowel_consonant_ratio(text),\n",
        "                self._get_capitalization_ratio(text),\n",
        "                self._get_double_char_freq(text)\n",
        "            ]\n",
        "            num_features.append(features)\n",
        "        return np.array(num_features)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        numerical_features = self._extract_num_features(X)\n",
        "        self.scaler.fit(numerical_features)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        numerical_features = self._extract_num_features(X)\n",
        "        return X, self.scaler.transform(numerical_features)\n",
        "\n",
        "# Wrapper to combine vectorizer and additional features\n",
        "class VectorizerWrapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.countvec = TfidfVectorizer(ngram_range=(1, 2), max_features=500)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        texts, _ = X\n",
        "        self.countvec.fit(texts)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        texts, numerical_features = X\n",
        "        return self.countvec.transform(texts), numerical_features\n",
        "\n",
        "# Transformer to convert sparse matrix to dense array\n",
        "class MatrixToArrayConverter(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return X[0].toarray(), X[1]\n",
        "\n",
        "# Transformer to unify text and numerical features into a single array\n",
        "class MatrixUnifier(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        unified = np.concatenate([X[0], X[1]], axis=1)\n",
        "        unified = unified.astype(np.float32)\n",
        "        return unified\n",
        "\n",
        "# Define the neural network module\n",
        "class ClassifierModule(nn.Module):\n",
        "    def __init__(self, input_size=600, num_units=200, num_classes=2, nonlin=F.relu, dropout=0.5):\n",
        "        super(ClassifierModule, self).__init__()\n",
        "        self.num_units = num_units\n",
        "        self.nonlin = nonlin\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.dense0 = nn.Linear(input_size, num_units)\n",
        "        self.nonlin = nonlin\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.dense1 = nn.Linear(num_units, 50)\n",
        "        self.output = nn.Linear(50, num_classes)\n",
        "\n",
        "    def forward(self, X, **kwargs):\n",
        "        X = self.nonlin(self.dense0(X))\n",
        "        X = self.dropout(X)\n",
        "        X = F.relu(self.dense1(X))\n",
        "        X = self.output(X)\n",
        "        return X.squeeze(dim=1)\n",
        "\n",
        "# Define input size and number of classes\n",
        "input_size = 503  # Adjusted to match the number of features\n",
        "num_classes = len(le_fitted.classes_)\n",
        "\n",
        "# Initialize the neural network classifier with Skorch\n",
        "net = NeuralNetClassifier(\n",
        "    ClassifierModule(input_size=input_size, num_classes=num_classes),\n",
        "    max_epochs=30,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    lr=0.1,\n",
        "    # device='cuda',  # Uncomment this to train with CUDA\n",
        ")\n",
        "\n",
        "# Create the pipeline\n",
        "pipe = Pipeline(steps=[\n",
        "    ('TextNormalizer', TextNormalizer()),\n",
        "    ('FeatureExtractor', FeatureExtractor()),\n",
        "    ('Vectorizer', VectorizerWrapper()),\n",
        "    ('MatrixToArrayConverter', MatrixToArrayConverter()),\n",
        "    ('MatrixUnifier', MatrixUnifier()),\n",
        "    ('net', net)\n",
        "], verbose=True)\n",
        "\n",
        "# Fit the model\n",
        "pipe.fit(train_df['text'].to_numpy(), y_train_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "preds = pipe.predict(test_df['text'].to_numpy())\n",
        "\n",
        "# Calculate F1 scores\n",
        "f1_micro = f1_score(preds, y_test_encoded, average='micro')\n",
        "f1_macro = f1_score(preds, y_test_encoded, average='macro')\n",
        "print(f'F1-micro-score on the test set: {f1_micro}')\n",
        "print(f'F1-macro-score on the test set: {f1_macro}')\n"
      ],
      "metadata": {
        "id": "S7tQOZgMKjUK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dyslexia-chinese",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}